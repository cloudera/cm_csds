// Licensed to Cloudera, Inc. under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  Cloudera, Inc. licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
{
  "name" : "SPARK",
  "label" : "Spark (Standalone)",
  "description" : "Apache Spark is an open source cluster computing system. This is the standalone version of the service which does not use YARN for resource management. Cloudera recommends using Spark on YARN instead of this standalone version.",
  "version" : "5.12.0",
  "runAs" : {
    "user" : "spark",
    "group" : "spark"
  },
  /* CM removes it from Express Wizard for CDH 5 */
  "inExpressWizard" : true,
  "icon" : "images/icon.png",
  "parcel" : {
    /* Automatically add the spark parcel repository */
    "repoUrl" : "https://archive.cloudera.com/spark/parcels/latest/",
    "requiredTags" : ["spark", "cdh"],
    "optionalTags" : ["spark-plugin"]
  },
  "serviceDependencies" : [
    {
      "name" : "HDFS",
      "required" : "false"
    }
  ],
  "commands" : [
    {
      "name" : "SparkUploadJarServiceCommand",
      "label" : "Upload Spark Jar",
      "description" : "Upload Spark assembly jar to HDFS.",
      "roleName" : "SPARK_MASTER",
      "roleCommand" : "SparkUploadJarCommand",
      "runMode" : "single"
    }
  ],
  "hdfsDirs" : [
    {
      "name" : "CreateSparkUserDirCommand",
      "label" : "Create Spark User Dir",
      "description" : "Creates the Spark user directory in HDFS.",
      "directoryDescription" : "Spark HDFS user directory",
      "path" : "/user/${user}",
      "permissions" : "0751"
    },
    {
      "name" : "CreateSparkHistoryDirCommand",
      "label" : "Create Spark History Log Dir",
      "description" : "Creates the directory in HDFS where application history will be stored.",
      "directoryDescription" : "Spark Application History directory",
      "path" : "${spark_history_log_dir}",
      "permissions" : "1777"
    }
  ],
  "serviceInit" : {
    "preStartSteps" : [
      {
        "commandName" : "CreateSparkUserDirCommand"
      },
      {
        "commandName" : "CreateSparkHistoryDirCommand"
      },
      {
        "commandName" : "SparkUploadJarServiceCommand"
      }
    ]
  },
  "parameters" : [
    {
      "name" : "identity_string",
      "label" : "Identity String",
      "description" : "The identity string of the Spark instance",
      "configName" : "identity.string",
      "required" : "false",
      "type" : "string"
    },
    {
      "name" : "spark_jar_hdfs_path",
      "label" : "Spark Jar Location (HDFS)",
      "description" : "The location of the Spark jar in HDFS. This configuration is for backwards compatibility and is not used by the Spark Standalone service.",
      "type" : "path",
      "pathType" : "serviceSpecific",
      "default" : ""
    },
    {
      "name" : "spark_history_log_dir",
      "label" : "Spark History Location (HDFS)",
      "description" : "The location of Spark application history logs in HDFS. Changing this value will not move existing logs to the new location.",
      "configName" : "spark.eventLog.dir",
      "default" : "/user/spark/applicationHistory",
      "type" : "string"
    }
  ],
  "rolesWithExternalLinks" : ["SPARK_MASTER", "SPARK_HISTORY_SERVER"],
  "roles" : [
    {
      "name" : "SPARK_MASTER",
      "label" : "Master",
      "pluralLabel" : "Masters",
      "jvmBased": true,
      "startRunner" : {
        "program" : "scripts/control.sh",
        "args" : [ "start_master" ],
        "environmentVariables" : {
          "SPARK_DAEMON_MEMORY" : "${master_max_heapsize}",
          "SPARK_IDENT_STRING" : "${user}",
          "SPARK_MASTER_PORT" : "${master_port}",
          "SPARK_MASTER_WEBUI_PORT" : "${master_webui_port}",
          "ADDITIONAL_ARGS" : "${master_additional_args}"
        }
      },
      "commands" : [
        {
          "name" : "SparkUploadJarCommand",
          "label" : "Upload Spark Jar",
          "description" : "Upload Spark assembly jar to HDFS.",
          "expectedExitCodes" : [0],
          "requiredRoleState" : "stopped",
          "commandRunner" : {
            "program" : "scripts/control.sh",
            "args" : [ "upload_jar" ],
            "environmentVariables" : {
              "SPARK_JAR" : "${spark_jar_hdfs_path}"
            }
          }
        }
      ],
      "externalLink" : {
        "name" : "master_web_ui",
        "label" : "Master Web UI",
        "url" : "http://${host}:${master_webui_port}"
      },
      "topology" : { "minInstances" : 1, "maxInstances" : 1 },
      "logging" : {
         "configFilename" : "spark-conf/log4j.properties",
         "dir" : "/var/log/spark",
         "filename" : "spark-master-${host}.log",
         "configName" : "log.dir",
         "modifiable" : true,
         "loggingType" : "log4j"
      },
      "parameters" : [
        {
          "name" : "master_address",
          "label" : "Master Address",
          "description" : "Overrides the address where the Master will listen for connections. Note: Spark will not work if the wildcard (0.0.0.0) address is used here.",
          "configName" : "server.address",
          "required" : "false",
          "type" : "string",
          "default" : ""
        },
        {
          "name" : "master_port",
          "label" : "Master Port",
          "description" : "The port of the master",
          "configName" : "server.port",
          "required" : "true",
          "type" : "port",
          "default" : 7077
        },
        {
          "name" : "master_webui_port",
          "label" : "Master WebUI Port",
          "description" : "The port of the master WebUI",
          "configName" : "webui.port",
          "required" : "true",
          "type" : "port",
          "default" : 18080
        },
        {
          "name" : "master_additional_args",
          "label" : "Additional Master args",
          "description" : "Additional arguments for the master",
          "configName" : "additional.args",
          "required" : "false",
          "type" : "string",
          "default" : ""
        },
        {
          "name" : "master_max_heapsize",
          "label" : "Java Heap Size of Master in Bytes",
          "description" : "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in bytes.",
          "required" : "true",
          "type" : "memory",
          "unit" : "bytes",
          "min" : 67108864,
          "default" : 536870912,
          "scaleFactor" : 1.3
        }
      ],
      "configWriter" : {
        "peerConfigGenerators" : [
          {
            "filename" : "spark-conf/master.properties",
            "params" : [ "master_address", "master_port" ],
            "roleName" : "SPARK_MASTER"
          }
        ],
        "auxConfigGenerators" : [
          {
            "filename" : "spark-conf/spark-env.sh",
            "sourceFilename" : "aux/client/spark-env.sh"
          }
        ]
      }
    },
    {
      "name" : "SPARK_WORKER",
      "label" : "Worker",
      "pluralLabel" : "Workers",
      "jvmBased": true,
      "startRunner" : {
        "program" : "scripts/control.sh",
        "args" : [ "start_worker" ],
        "environmentVariables" : {
          "SPARK_DAEMON_MEMORY" : "${worker_max_heapsize}",
          "SPARK_IDENT_STRING" : "${user}",
          "SPARK_WORKER_PORT" : "${worker_port}",
          "SPARK_WORKER_WEBUI_PORT" : "${worker_webui_port}",
          "SPARK_WORKER_DIR" : "${work_directory}",
          "SPARK_WORKER_MEMORY" : "${executor_total_max_heapsize}",
          "ADDITIONAL_ARGS" : "${worker_additional_args}"
        }
      },
      "externalLink" : {
       "name" : "worker_web_ui",
       "label" : "Worker Web UI",
       "url" : "http://${host}:${worker_webui_port}"
      },
      "topology" : { "minInstances" : 1 },
      "logging" : {
        "configFilename" : "spark-conf/log4j.properties",
        "dir" : "/var/log/spark",
        "filename" : "spark-worker-${host}.log",
        "modifiable" : true,
        "configName" : "log.dir",
        "loggingType" : "log4j"
      },
      "parameters" : [
        {
          "name" : "worker_port",
          "label" : "Worker Port",
          "description" : "The port of the worker",
          "configName" : "server.port",
          "required" : "true",
          "type" : "port",
          "default" : 7078
        },
        {
          "name" : "worker_webui_port",
          "label" : "Worker WebUI Port",
          "description" : "The port of the worker WebUI",
          "configName" : "webui.port",
          "required" : "true",
          "type" : "port",
          "default" : 18081
        },
        {
          "name" : "worker_additional_args",
          "label" : "Additional Worker args",
          "description" : "Additional arguments for the workers",
          "configName" : "additional.args",
          "required" : "false",
          "type" : "string",
          "default" : ""
        },
        {
          "name" : "work_directory",
          "label" : "Work directory",
          "description" : "The work directory for temporary files",
          "configName" : "work.dir",
          "required" : "true",
          "type" : "path",
          "pathType" : "localDataDir",
          "default" : "/var/run/spark/work"
        },
        {
          "name" : "worker_max_heapsize",
          "label" : "Java Heap Size of Worker in Bytes",
          "description" : "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in bytes.",
          "required" : "true",
          "type" : "memory",
          "unit" : "bytes",
          "min" : 67108864,
          "default" : 536870912,
          "scaleFactor" : 1.3
        },
        {
          "name" : "executor_total_max_heapsize",
          "label" : "Total Java Heap Sizes of Worker's Executors in Bytes",
          "description" : "Memory available to the Worker's Executors. This is the maximum sum total of all the Executors' Java heap sizes on this Worker node. Passed to Java -Xmx. Measured in bytes.",
          "required" : "true",
          "type" : "memory",
          "unit" : "bytes",
          "min" : 536870912,
          "default" : 8589934592,
          "scaleFactor" : 1.3,
          "autoConfigShare" : 100
        }
      ],
      "configWriter" : {
        "peerConfigGenerators" : [
          {
            "filename" : "spark-conf/master.properties",
            "params" : [ "master_address", "master_port" ],
            "roleName" : "SPARK_MASTER"
          }
        ],
        "auxConfigGenerators" : [
          {
            "filename" : "spark-conf/spark-env.sh",
            "sourceFilename" : "aux/client/spark-env.sh"
          }
        ]
      },
      "cgroup" : {
        "cpu" : {
          "autoConfigured" : true
        },
        "blkio" : {
          "autoConfigured" : true
        }
      }
    },
    {
      "name" : "SPARK_HISTORY_SERVER",
      "label" : "History Server",
      "pluralLabel" : "History Servers",
      "jvmBased": true,
      "startRunner" : {
        "program" : "scripts/control.sh",
        "args" : [ "start_history_server" ],
        "environmentVariables" : {
          "HISTORY_LOG_DIR" : "${spark_history_log_dir}",
          "SPARK_DAEMON_MEMORY" : "${history_server_max_heapsize}",
          // TODO: move to command line args History Server enhancements are backported.
          "SPARK_DAEMON_JAVA_OPTS" : "-Dspark.history.ui.port=${history_server_web_port}"
        }
      },
      "externalLink" : {
        "name" : "history_server_web_ui",
        "label" : "History Server Web UI",
        "url" : "http://${host}:${history_server_web_port}"
      },
      "topology" : { "minInstances" : 0, "maxInstances" : 1 },
      "logging" : {
        "configFilename" : "spark-conf/log4j.properties",
        "dir" : "/var/log/spark",
        "filename" : "spark-history-server-${host}.log",
        "modifiable" : true,
        "loggingType" : "log4j"
      },
      "parameters" : [
        {
          "name" : "history_server_web_port",
          "label" : "History Server WebUI Port",
          "configName" : "history.port",
          "description" : "The port of the history server WebUI",
          "required" : "true",
          "type" : "port",
          "default" : 18088
        },
        {
          "name" : "history_server_max_heapsize",
          "label" : "Java Heap Size of History Server in Bytes",
          "description" : "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in bytes.",
          "required" : "true",
          "type" : "memory",
          "unit" : "bytes",
          "min" : 67108864,
          "default" : 536870912
        }
      ],
      "configWriter" : {
        "auxConfigGenerators" : [
          {
            "filename" : "spark-conf/spark-env.sh",
            "sourceFilename" : "aux/client/spark-env.sh"
          }
        ]
      }
    }
  ],
  "gateway" : {
    "alternatives" : {
      "name" : "spark-conf",
      "priority" : 50,
      "linkRoot" : "/etc/spark"
    },
    "parameters" : [
      {
        "name" : "spark_history_enabled",
        "label" : "Enable History",
        "description" : "Write Spark application history logs to HDFS.",
        "configName" : "spark.eventLog.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_python_path",
        "label" : "Extra Python Path",
        "description" : "Python library paths to add to PySpark applications.",
        "required" : "false",
        "type" : "path_array",
        "pathType" : "serviceSpecific",
        "separator" : ":",
        "default" : [ ]
      },
      {
        "name" : "spark_gateway_ui_kill_enabled",
        "label" : "Enable Kill From UI",
        "description" : "Whether to allow users to kill running stages from the Spark Web UI.",
        "configName" : "spark.ui.killEnabled",
        "required" : "true",
        "type" : "boolean",
        "default" : true
      }
    ],
    "scriptRunner" : {
      "program" : "scripts/control.sh",
      "args" : [ "client" ],
      "environmentVariables" : {
        "SPARK_JAR" : "${spark_jar_hdfs_path}",
        "PYTHON_PATH" : "${spark_python_path}"
      }
    },
    "logging" : {
      "configFilename" : "spark-conf/log4j.properties",
      "loggingType" : "log4j",
      "additionalConfigs" : [
        { "key" : "log4j.logger.org.eclipse.jetty", "value" : "WARN" },
        { "key" : "log4j.logger.org.spark-project.jetty", "value" : "WARN" },
        { "key" : "log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle", "value" : "ERROR" },
        { "key" : "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper", "value" : "INFO" },
        { "key" : "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", "value" : "INFO" },
        { "key" : "log4j.logger.org.apache.parquet", "value" : "ERROR" },
        { "key" : "log4j.logger.parquet", "value" : "ERROR" },
        { "key" : "log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler", "value" : "FATAL" },
        { "key" : "log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry", "value" : "ERROR" }
      ]
    },
    "configWriter" : {
      "generators" : [
        {
          "filename" : "spark-conf/spark-defaults.conf",
          "configFormat" : "properties",
          "includedParams" : [
            "spark_history_enabled",
            "spark_history_log_dir",
            "spark_gateway_ui_kill_enabled"
          ]
        }
      ],
      "auxConfigGenerators" : [
        {
          "filename" : "spark-conf/spark-env.sh",
          "sourceFilename" : "aux/client/spark-env.sh"
        }
      ],
      "peerConfigGenerators" : [
        {
          "filename" : "spark-conf/master.properties",
          "params" : [ "master_address", "master_port" ],
          "roleName" : "SPARK_MASTER"
        },
        {
          "filename" : "spark-conf/history.properties",
          "params" : ["history_server_web_port"],
          "roleName" : "SPARK_HISTORY_SERVER"
        }
      ]
    }
  }
}


